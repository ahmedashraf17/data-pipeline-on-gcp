{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "This code sets up the data pipeline to fetch the data from an API,\n",
    "store it in GCS, transform the data with Cloud Dataflow,\n",
    "load the transformed data into BigQuery,\n",
    "and schedule the pipeline using Cloud Scheduler. \n",
    "'''\n",
    "\n",
    "# Ingest data from the API and store the data in GCS:\n",
    "\n",
    "\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "\n",
    "# Make API call to fetch data\n",
    "response = requests.get('https://api.example.com/data')\n",
    "\n",
    "# Store data in GCS\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket('my-bucket-name')\n",
    "blob = bucket.blob('data.json')\n",
    "blob.upload_from_string(response.content)\n",
    "\n",
    "\n",
    "# Transform the data using Cloud Dataflow:\n",
    "\n",
    "\n",
    "import apache_beam as beam\n",
    "from google.cloud import storage\n",
    "\n",
    "# Define the pipeline\n",
    "class MyPipeline:\n",
    "    def run(self, input_path, output_path):\n",
    "        with beam.Pipeline() as p:\n",
    "            (p\n",
    "             | 'ReadData' >> beam.io.ReadFromText(input_path)\n",
    "             | 'TransformData' >> beam.Map(self.transform)\n",
    "             | 'WriteData' >> beam.io.WriteToText(output_path))\n",
    "\n",
    "    def transform(self, data):\n",
    "        # Perform data transformation\n",
    "        return {'new_key': data['old_key']}\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline = MyPipeline()\n",
    "pipeline.run('gs://my-bucket-name/data.json', 'gs://my-bucket-name/transformed_data')\n",
    "\n",
    "\n",
    "# Load the transformed data into BigQuery:\n",
    "\n",
    "```\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Load data into BigQuery\n",
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('my_dataset')\n",
    "table_ref = dataset_ref.table('my_table')\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "uri = 'gs://my-bucket-name/transformed_data-*'\n",
    "load_job = client.load_table_from_uri(uri, table_ref, job_config=job_config)\n",
    "load_job.result()\n",
    "```\n",
    "\n",
    "# Schedule the pipeline using Cloud Scheduler:\n",
    "\n",
    "```\n",
    "from google.cloud import scheduler_v1\n",
    "from google.protobuf import timestamp_pb2\n",
    "\n",
    "# Define the job to run the pipeline\n",
    "project_id = 'my-project-id'\n",
    "location = 'us-central1'\n",
    "input_path = 'gs://my-bucket-name/data.json'\n",
    "output_path = 'gs://my-bucket-name/transformed_data'\n",
    "target_url = 'https://<REGION>-<PROJECT_ID>.cloudfunctions.net/run_pipeline'\n",
    "schedule = '0 * * * *'\n",
    "time_zone = 'America/Los_Angeles'\n",
    "\n",
    "client = scheduler_v1.CloudSchedulerClient()\n",
    "parent = f\"projects/{project_id}/locations/{location}\"\n",
    "\n",
    "job = {\n",
    "    'name': 'my-pipeline',\n",
    "    'http_target': {\n",
    "        'uri': target_url,\n",
    "        'http_method': 'POST'\n",
    "    },\n",
    "    'schedule': schedule,\n",
    "    'time_zone': time_zone,\n",
    "    'user_update_time': timestamp_pb2.Timestamp(),\n",
    "}\n",
    "\n",
    "# Create the job\n",
    "response = client.create_job(\n",
    "    request={\n",
    "        \"parent\": parent,\n",
    "        \"job\": job,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the job name and schedule\n",
    "print(\"Created job:\", response.name)\n",
    "print(\"Schedule:\", response.schedule)\n",
    "\n",
    "\n",
    "\n",
    "#Note that you will need to replace `<REGION>` and `<PROJECT_ID>` with the appropriate values for your project, and also update the `target_url` to point to the appropriate Cloud Function that runs the data pipeline. Additionally, you will need to set up authentication and ensure that the necessary APIs are enabled in your GCP project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
